{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Cleaning methods\n",
    "\n",
    "## List of methods that will be tried out \n",
    "\n",
    "<ol>\n",
    "    <li>Tokenizing</li>\n",
    "    <li>Stop-word removal</li>\n",
    "    <li>Stemming</li>\n",
    "    <li>Lemmatization</li>\n",
    "</ol>\n",
    "\n",
    "Also make note of how these methods will be used in the DevSpec system and what concerns arise with the usage of these methods for each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stop word removal and Tokenizing\n",
    "\n",
    "Library used <b>NLTK</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'document', 'which', 'represents', 'a', 'discussion', 'thread', 'the', 'structuring', 'of', 'a', 'discussion', 'thread', 'has', 'not', 'been', 'decided', 'yet', 'therefore', 'it', 'is', 'of', 'utmost', 'importance', 'to', 'decide', 'on', 'a', 'structure', 'and', 'the', 'number', 'of', 'comments', 'that', 'needs', 'to', 'be', 'considered', 'when', 'considering', 'a', 'discussion', 'forum']\n"
     ]
    }
   ],
   "source": [
    "#Import NLTK and stopwords and cast it to a set.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#stopwords.words('english') returns the *list* of stop words it is casted to a *set* \n",
    "stop_word_list = set(stopwords.words('english'))\n",
    "#print(len(stop_word_list))\n",
    "\n",
    "\n",
    "sample_doc = (\"This is a sample document which represents a discussion thread the structuring of a discussion \"\n",
    "\"thread has not been decided yet therefore it is of utmost importance to decide on a structure and \"\n",
    "\"the number of comments that needs to be considered when considering a discussion forum\")\n",
    "\n",
    "#import tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#*list* of tokenized words\n",
    "#type(word_tokens) is list\n",
    "word_tokens = word_tokenize(sample_doc)\n",
    "print(word_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'document', 'represents', 'discussion', 'thread', 'structuring', 'discussion', 'thread', 'decided', 'yet', 'therefore', 'utmost', 'importance', 'decide', 'structure', 'number', 'comments', 'needs', 'considered', 'considering', 'discussion', 'forum']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_word_list: \n",
    "        filtered_sentence.append(w) \n",
    "  \n",
    "#print(word_tokens) \n",
    "print(filtered_sentence) \n",
    "#type(filtered_sentence) is list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concerns in tokenizing and stop word removal\n",
    "\n",
    "<ul>\n",
    "    <li>Stop word removal could inturrupt user intention identification component</li>\n",
    "    <li>Performance concerns with implementing lists and sets</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (topicModeling)",
   "language": "python",
   "name": "topicmodeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
